---
layout: post
title: (자료구조)Map
categories: [data]
excerpt: ' '
comments: false
share: false
tags: data
date: 2019-05-17
---

## Maps

Map은 검색이 가능한 구조다. -> 따라서 키값이 중복되지 않음
주로 키값 자체를 데이터를 저장하는 주소값과 연관시켜서 설정한다 -> **associative store**

### Map Operations

- integer size(), boolean isEmpty()
- Object get(k) -> k를 검색해서 해당 value 값을 리턴
- Object put(k,v) -> entry(k, v)를 insert하고,
  - 만약 k가 처음 들어온거면 null 리턴
  - k가 중복되었다면 entry변경하고, old value를 리턴
- Object remove(k) -> key k를 가진 entry 삭제. 그리고 value 리턴
- list keys() -> 키 값만 모아서 리턴
- list values() -> value 값만 모아서 리턴
- list entries() -> 리스트 리턴

![No Image](/assets/posts/20190517/1.png)

## A Simple List-Based Map

![No Image](/assets/posts/20190517/2.png)

### Searching Algorithm

1. 모든 노드를 하나식 가져온다.
2. 노드의 element에 저장되어 있는 key값이 내가 찾는 key값과 같은지 비교
3. 같으면 value값 리턴

![No Image](/assets/posts/20190517/3.png)

### Insertion Algorithm

앞의 포문이 있는 이유는, 중복성을 검사해주기 위해서다.
중복된게 없으면 추가한다. n은 엔트리의 개수다.

![No Image](/assets/posts/20190517/4.png)

### Deletion Algorithm

앞의 포문은 k값을 찾는다.
찾아서 해당 엔트리 remove 한 뒤
n값 (전체 엔트리 갯수)를 감소시킨다.

![No Image](/assets/posts/20190517/5.png)

### Performance of List-based Map

- put,get,remove operations 값은 O(n)time이 걸린다.
- 작은 map size에 적합

## Hash Table

- 해쉬 테이블으 키값은 주로 엔트리의 주소값으로 활용된다.
- 찾는 데 있어서 속도가 빠르다.
- `java.util.HashMap`으로 사용 가능

해쉬 테이블을 만들기 위해서는 2가지 요소 필요

- Bucket array : 여기에 실제 데이터가 저장
- Hash function : 0~n-1 사이의 값으로 바꿔주는 함수

## Evaluating Hash Function

1. 해쉬코드 생성 : key값이 정수가 아닌 경우, 임의의 int형 해쉬 코드로 바꿔주는 단계
2. 해쉬코드 압축 : 나온 정수형 해쉬 코드를 bucket array의 크기에 맞게 줄여주는 단계

## Hash Code Generation Method

1. key 값이 Object일 때 : Memory address reinterpretation (메모리 주소를 hashcode로 사용하겠다)
2. key 값이 숫자일 때

   - Single Integer : 해쉬 코드 생성이 필요하지 않아
   - Not a single Integer(ex : byte, short, long, float, double, array of int ... )
     - Shorter than single int : integer casting
     - Longer than single int : component summation

3. 키값이 strings 일 때,
   - Polynomial accumulation
   - Cyclic shift

## Hash Code for Object Keys

```java
// Example #1
FileInputStream f1 = new FileInputStream (“data.txt”);
FileInputStream f2 = new FileInputStream (“data.txt”);
System.out.println(f1.hashCode() == f2.hashCode());
// Example #2
String s1 = new String(“data.txt”);
String s2 = new String(“data.txt”);
System.out.println( s1.hashCode() == s2.hashCode());
// Example #3
Integer i1 = new Integer(123);
Integer i2 = new Integer(123);
System.out.println(i1.hashCode() == i2.hashCode());
```

각각 f1과 f2이라는 오브젝트를 생성하게 되면, 메모리에 오브젝트가 저장되는 공간이 할당된다.
메모리의 특정한 주소에 그 오브젝트가 생긴다는 것.
그 주소값 자체가 정수이므로 그 주소값을 hashcode로 쓴다는 idea => 이게 address limit repretation

자바 메뉴얼을 보면 메소드 이름이 다 나오는데, 그 중 해쉬코드라는 메소드는 모두 있다.
이게 가장 상위에서 상속받는 메소드.

해쉬코드 method => Object의 해쉬 코드 값을 생성해주는 함수
이게 기본적으로 java address limit repretation 방식을 사용
따라서 어떤 Object 생성한 뒤, 해쉬코드 메소드 호출 시 -> integer 값이 반환된다.

## Hash Code for Numeric Keys

#### Integer Casting : 4바이트 이하인 경우

- byte나 short같은 경우 casting을 해주면 4바이트 인티저로 만들기 쉽다.
- float같은 경우 4바이트지만 소수점을 가지고 있다. 따라서 해쉬코드를 쓸 수 없어 -> 별도로 해쉬코드 생성 필요
  - 1. 강제로 static 캐스팅을 해준다 -> 123.4 or1 123.5라고 해보자. 123.4 이하의 소수점 정보가 다 사라진다. => **Collision**이 발생
  - 2. `Float.floatToIntBits(x)` => 123.4라는 숫자를 비트로 표현하고 그걸 정수값을 바꿈. -> 완전히 다른 값. 따라서 소수점 아래까지 살릴 수 있음

#### Component summation : 4바이트 보다 클 경우

- 가장 쉬운 건 casting -> 앞의 4바이트가 사라지는 문제
- 8바이트를 4바이트, 4 바이트로 쪼갠 뒤 그걸 summation 한다. 그러면 살아남. **문제 : 오버플로우 발생 가능**
- 오버플로우가 나서 사라지는 정보는 어쩔 수 없다.
- 문자열에도 응용 가능. ex) stop -> 캐릭터 4개가 모여있는 것 -> 캐릭터 하나하나는 각각의 유니코드. -> s,t,o,p 각각 더한 것으로 해쉬 코드 생성 가능
- 위 방법 문제 : stop, tops, spot모두 동일한 해쉬코드 값을 가짐.

## Hash Code for String Keys

![No Image](/assets/posts/20190517/6.png)

#### Polynomial accumulation(다항식 축적)

- 문자열을 그냥 더하는게 아니라 가중치를 곱해서 더하자! -> 즉 일반적인 summation이 아니라 weighted summation
- 이런 방법을 polymomial accumulatoin(다항식 축적)이라 한다.
- a에 들어가는 값이 매우 중요하며 33,37,39,41 중 하나를 사용하면 50000개 중 collision 발생함 -> 7번 즉 0.0014%

#### Cyclic shift

- 개념적으로는 Polynomial과 동일, 차이점은 a라는 정수로 weighted sum을 계산하는 것이 아니라, 곱하는 연산 대신 shift 사용
- ex) stop이란 단어가 있다. 얘를 해쉬 코드를 만든다고 가정하자
- integer h를 하나 선언(4바이트) -> s를 더해준 뒤 왼쪽으로 5비트 만큼 shift 한다.
- 마찬가지로 t, o, p를 더해준 뒤 더할 때마다 왼쪽으로 5비트 만큼 shift
- 32비트가 모자르면 -> 오른쪽으로 넘어가서 더해준다.

#### Compression Function

- 0부터 n-1 사이의 범위로 압축을 해줘야해. 그게 컴프레션 함수
- 가장 쉬운 방법은 mod연산. `mod N`을 해주면 0과 N-1 사이의 범위로 압축이 된다.
- N 값을 선택할 때는 prime number(소수)여야 한다. 그래야 collision을 줄일 수 있다.
- `h(x) = ((a*x+b)%p)%N` MAD(Multiply, ADD & Divide) Method를 사용하면 콜리젼을 줄일 수 있어. (수식에 대한 증명은 패스)
- 여기서 p는 N 보다 큰 소수를 말한다.
- a와 b는 [0, p-1]에 해당하는 수를 말한다.

## Collision Handling

- 해쉬코드를 만들고, 컴프레션 함수로 압축하고 그래서 버켓 어레이에 저장해도. 콜리션을 생길 수밖에 없어 당연히.
- 키값이 가질 수 있는 범위가, 해쉬테이블이 가질 수 있는 범위보다 훨씬 크기 때문

## Separate Chaining

![No Image](/assets/posts/20190517/7.png)

- bucket array에 직접 저장하면, 하나 밖에 저장을 못하기 때문에
- 각각의 array가 조그만 list의 주소값을 가진다. -> 컬리젼이 생기면 리스트에 추가됌
- 단점: 리스트로 되어있는 조그만 리스트 map에서 다시 한번 더 get을 해야하기 때문에 메모리를 많이 쓴다.
- 단점보완 : 미리 리스트를 가지고 있을 필요가 없다. 리스트에 들어오면 생성되는 방법으로.

## Map Operations with Separate Chaining

![No Image](/assets/posts/20190517/8.png)

- 이미 해쉬테이블을 가지고 있고, 리스트 기반의 맵을 만들었다고 가정
- get(k)하게 되면 array의 케이번째 위치에서 또 다른 맵이 나온다. 거기서 또 한번 겟을 하면 된다.
- 풋과 리무브도 이와 같은 방식이다.

## Analysis of Separate Chaining

- bucket Array의 크기를 N이라고 가정하자.
- 해쉬 테이블에 저장해야 할 전체 데이터의 갯수가 n이라고 가정하자
- 각 셀에 필요한 최소한의 리스트의 크기는 n/M이다.
- 위 예제로 살펴보면 n은 3, N은 5가 될 것이다.
- 평균적으로 이정도의 크기가 있어야 모든 collision을 처리해줄 수 있다.
- 각각의 get, put, remove operations이 작동하는 시간 -> O(n/N)이다. (이건 평균 시간)
- 만약 리스트의 크기가 고정되어 있다면, 예를 들어 collision 최대 5라고 한다면, 시간 복잡도가 O(n/N)이라면 상수타임이 된다.

## Collision Handling with Open Addressing

- separate chaining은 bucket array에다가 list를 매달아서 사용
- 메모리를 많이 사용한다는 단점을 보완하기 위해 bucket array만으로 다 해결하기.
- 이 방법은 메모리는 덜 쓰지만 계산량이 많다. -> 3가지 방법 존재
- Linear probing
- Quadratic probing
- Double hashing

### Linear Probing

![No Image](/assets/posts/20190517/9.png)

- h(x)위치에서 collision이 발생했다면, h(x)+1에 시도 안되면 +2에 시도.
- 단점 : 해쉬테이블의 데이터들이 한 군데에 뭉쳐지는 현상 -> **cluster**현상 발생 => collision을 촉진시킴

## Quadratic Probing

![No Image](/assets/posts/20190517/10.png)

- cluster가 생기는 이유 -> linear하게 찾기 때문
- A[h(k)]에 collision이 발생하면, j가 아닌 j^2을 더하자.
- 단점 : 조그만 cluster가 여러 군데 생긴다. 뿐만 아니라 해쉬 테이블의 공간이 반 이상 남았는데도 빈칸을 못 찾는 경우가 발생.

## Doubling Hashing

![No Image](/assets/posts/20190517/11.png)

- 성능이 가장 좋지만, 계산을 두번 해야한다.
- 해쉬 함수를 두개 사용
- collision이 생길 경우, j를 더하는게 아니라, j\*h'(k)를 해준다.(secondary hash function을 j에 곱해서 더해준다.)
- secondary hash function은 `h'(k) = q - (k mod q)`으로 만든다. 이 때 prime number q는 N보다 작다.

### Example of Doubling Hashing

![No Image](/assets/posts/20190517/12.png)

- 세컨더리 해쉬 함수 값은 절대 0이 되서는 안된다.
- 따라서 h'(k)에 collision이 생기면, +1을 더해서 mod N을 하며
- 또 추돌이 생길 경우 +2를 해준다. 점점 숫자를 늘려간다

## Load Factor of Hash Table

- `Load Factor = n/N` - 저장해야할 데이터의 갯수 / 버킷 어레이의 크기
- Load Factor > 1이면 무조건 충돌이다.
- separate Chaning의 경우 < 0.9 일 때, 컬리젼이 덜 생기고
- open addressing의 경우 < 0.5 로 유지시켜주는 게 좋다. (경험적으로)
- 따라서 bucket array를 모니터링 하고 있다가, separate chaining이다라고 한다면, 0.9보다 커지면 bucket array를 늘려줘야 한다. -> 이걸 rehashing이라고 부른다

## Rehashing

![No Image](/assets/posts/20190517/12.png)

1. 새로운 new bucket Array를 2N 사이즈로 만든다
2. 새로운 hash 함수를 정의한다 (h(k) = k mod 2N)
3. 이전 bucket array의 값을 새로운 bucket array로 옮긴다.

---

하영국 교수님 자바 수업
